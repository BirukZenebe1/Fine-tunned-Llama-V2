# Overview
This project focuses on fine-tuning a pre-trained large language model to improve formating on task-specific data. The goal is to improve response formatting and improve the model knowledge on the medical term domain.
# Key Features
Supervised fine-tuning on custom datasets
Parameter-efficient fine-tuning techniques (e.g., LoRA / adapters)
Model evaluation before and after fine-tuning
# Technologies
Python
PyTorch / Hugging Face Transformers
Pre-trained LLMs
GPU-accelerated training (e.g., Google Colab)
